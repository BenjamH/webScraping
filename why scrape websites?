why scrape websites?
website owner keeps website more up to date than their API
no rate-limiting
and only track through ip address, not api. and can spoof ip address.

1. find endpoint for fetching data
2. deal with pagination if necessary (can change the pagination to show 200 instead of 100)
3. monitor ajax response if framework in use (instead of using barebones css hooks, may have to go into ajax response, then would be basically doing API stuff)
4. use CSS hooks to find relevant data (Cheerio) can manipulate like html.
5. Do stuff..
Horseman is for navigating through a website.

copy the jquery code, the entire text, and past it into console of craigslist page.

then use jquery to find all tags with a certain class.
$('a.hdrlnk')

then call .length to see if matches the pagination.

request-promise is basically ajax, but can used in node enviornment.

cheerio littlerally just allows you to do jquery stuff.

for SPA apps, without url changing, and only certain elements changing through ajax request.



phantomJS can setup chronjobs from terminal. no browser required, used for testing (end to end test from one page to another does functioning tests) and data mining. way faster than without browser.

banjo mines social media websites for data. to protect future events.

any task you want to automate that need to on a site, just use horseman.

capcha, robots. (can do it through. queue, you're the human. the cc info automated.). apply jobs automated through lever. cover letter.

ask ron evenstevens website. and how to make terminal green.

front-end lectures.